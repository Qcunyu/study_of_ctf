# Robots 排除标准
##### `robots.txt` 是网站根目录下的一个文本文件，用于告知网络爬虫哪些内容可以或不可以抓取.
### **是"守则"但不是"法律"**

- ✅ **是通用守则**：绝大多数正规搜索引擎爬虫都会遵守
    
- ❌ **不是强制法律**：恶意爬虫可以完全无视它
    
- ⚠️ **没有技术阻挡**：它只是告诉爬虫"请不要"，而不是"不能"