# Robots 排除标准
 **`robots.txt` 是网站根目录下的一个文本文件，用于告知网络爬虫哪些内容可以或不可以抓取.**

### **是"守则"但不是"法律"**

- ✅ **是通用守则**：绝大多数正规搜索引擎爬虫都会遵守
    
- ❌ **不是强制法律**：恶意爬虫可以完全无视它
    
- ⚠️ **没有技术阻挡**：它只是告诉爬虫"请不要"，而不是"不能"


## eg.
```
User-agent: *          # 规则1：适用于所有爬虫
Disallow: /f10g.php    # 禁止所有爬虫抓取 /f10g.php

User-agent: Yandex     # 规则2：只适用于Yandex爬虫
Disallow: *            # 禁止Yandex抓取整个网站
```